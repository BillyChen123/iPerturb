{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iPerturb: An Introduction\n",
    "\n",
    "## Overview\n",
    "\n",
    "iPerturb is an efficient tool for integrating single-cell RNA sequencing (scRNA-seq) data with multiple samples and multiple conditions, focusing on removing batch effects while retaining condition-specific changes in gene expression. This document will introduce how to use iPerturb to process and analyze scRNA-seq datasets from different experimental conditions.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "We applied iPerturb to analyze droplet-based scRNA-seq data from peripheral blood mononuclear cells (PBMCs). The dataset consists of two groups: one group includes peripheral blood cells treated with interferon-β (INF-β), and the other group includes untreated control cells. You can download the dataset from [here](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE96583).\n",
    "\n",
    "Specifically, gene expression levels were measured from 8 experimental samples treated with INF-β (stimulated group; N = 7466 cells) and 8 control samples (control group; N = 6573 cells) to assess condition-specific changes in gene expression.\n",
    "\n",
    "## Loading\n",
    "\n",
    "Let's start by loading the required packages. We import the `iPerturb.iperturb` package as `iPerturb`. We also need to import the following two packages to ensure iPerturb works properly:\n",
    "\n",
    "- `scanpy`: iPerturb is built on the `scanpy` framework and accepts single-cell data files in the h5ad format.\n",
    "- `torch`: iPerturb uses `pytorch` to build the variational autoencoder and use `cuda` to accelerate the inference, we need to detect if cuda is avaliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is not available\n"
     ]
    }
   ],
   "source": [
    "import iperturb as iPerturb\n",
    "import torch\n",
    "import scanpy as sc\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print('cuda is available')\n",
    "else:\n",
    "    print('cuda is not available')\n",
    "\n",
    "anndata = sc.read_h5ad('/data/chenyz/iPerturb_project/data/Pbmc.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Data preprocessing of `anndata` included the following steps:\n",
    "\n",
    "1. **Quality Control**: Removal of low-quality cells and genes (default: min_genes=200, min_cells=3).\n",
    "   \n",
    "2. **Normalization**: Standardizing gene expression data (default: `normalize_total()`, `log1p()`).\n",
    "   \n",
    "3. **Dataset initiation**: Batch, condition, and groundtruth (optional) information are added to `anndata.obs` and set as category types.\n",
    "\n",
    "4. **Find highly variable genes**: Annotate highly variable genes to accelerate integration (default: n_top_genes=4000).\n",
    "\n",
    "\n",
    "In iPerturb, we provide a unified function `preprocess.data_load()` to accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenyz/anaconda3/envs/MOBA_Billy/lib/python3.10/site-packages/scanpy/preprocessing/_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    }
   ],
   "source": [
    "batch_key = 'batch_2'\n",
    "condition_key = 'batch'\n",
    "groundtruth_key = 'groundtruth'\n",
    "datasets,raw,var_names,index_names = iPerturb.preprocess.data_load(anndata, batch_key = batch_key ,condition_key = condition_key , groundtruth_key = groundtruth_key ,n_top_genes = 4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initiating\n",
    "After preprocessing the data, the next step is to initiate the model. Here are the steps to set up and initiate the iPerturb model:\n",
    "\n",
    "1. **Create Hyperparameters**: We start by creating the hyperparameters for the model using the `utils.create_hyper()` function.\n",
    "\n",
    "2. **Define Training Parameters**: We define the training parameters including the number of epochs and the optimizer. In this example, we use the Adam optimizer.\n",
    "\n",
    "3. **Initialize the Model**: Next, we initialize the iPerturb model using the `model.model_init_ function()`. This function sets up the model with the specified hyperparameters, latent dimensions, optimizer, learning rate, and other parameters. Here are the explanation of Parameters:\n",
    "\n",
    "- `hyper`: The hyperparameters created in step 1.\n",
    "   \n",
    "- `latent_dim1`, `latent_dim2`, `latent_dim3`: Dimensions of the latent variable of Z, Z_t and Z_s.\n",
    "   \n",
    "- `optimizer`: The optimizer used for training, in this case, Adam.\n",
    "   \n",
    "- `lr`: Learning rate for the optimizer.\n",
    "   \n",
    "- `gamma`: Learning rate decay factor.\n",
    "   \n",
    "- `milestones`: Epochs at which the learning rate is decayed.\n",
    "   \n",
    "- `set_seed`: Random seed for reproducibility.\n",
    "   \n",
    "- `cuda`: Boolean indicating whether to use GPU for training.\n",
    "   \n",
    "- `alpha`: Regularization parameter.\n",
    "\n",
    "Finally, we got 3 key component as output to strat VAE infernce(reference by [Pyro](https://pyro.ai/)):\n",
    "- `svi`: Stochastic Variational Inference (SVI) object used for optimizing the variational inference objective in the variational autoencoder (VAE) model.\n",
    "  \n",
    "- `scheduler`: Learning rate scheduler that adjusts the learning rate during training based on the specified milestones and gamma.\n",
    "  \n",
    "- `iPerturb_model`: The initialized iPerturb model, which includes the variational autoencoder (VAE) architecture configured with the specified hyperparameters and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### iPerturb\n",
    "hyper = iPerturb.utils.create_hyper(datasets, var_names, index_names)\n",
    "# 训练模型\n",
    "epochs = 30\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "svi, scheduler, iPerturb_model = iPerturb.model.model_init_(hyper, latent_dim1=100, latent_dim2=20, latent_dim3=20, \n",
    "                                                            optimizer=optimizer, lr=0.006, gamma=0.2, milestones=[20], \n",
    "                                                            set_seed=123, cuda=cuda, alpha=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Once the iPerturb model is initialized, we proceed to train the model using the `model.RUN()` function. The parameter `if_likelihood` is used to compute the model's t_logits. The model returns two results: `x_pred`, which represents the corrected matrix, and `reconstruct_data`, which represents the corrected AnnData.\n",
    "\n",
    "iPerturb is computationally efficient, with a typical runtime of approximately 5-10 minutes in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenyz/anaconda3/envs/MOBA_Billy/lib/python3.10/site-packages/pyro/util.py:303: UserWarning: Found vars in model but not guide: {'t_logit'}\n",
      "  warnings.warn(f\"Found vars in model but not guide: {bad_sites}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 00]  Loss: 2.91699\n",
      "[Epoch 01]  Loss: 2.12148\n",
      "[Epoch 02]  Loss: 1.59471\n",
      "[Epoch 03]  Loss: 1.35417\n",
      "[Epoch 04]  Loss: 1.24790\n",
      "[Epoch 05]  Loss: 1.19133\n",
      "[Epoch 06]  Loss: 1.15570\n",
      "[Epoch 07]  Loss: 1.13036\n",
      "[Epoch 08]  Loss: 1.11083\n",
      "[Epoch 09]  Loss: 1.09498\n",
      "[Epoch 10]  Loss: 1.08158\n",
      "[Epoch 11]  Loss: 1.07002\n",
      "[Epoch 12]  Loss: 1.05984\n",
      "[Epoch 13]  Loss: 1.05086\n",
      "[Epoch 14]  Loss: 1.04273\n",
      "[Epoch 15]  Loss: 1.03514\n",
      "[Epoch 16]  Loss: 1.02849\n",
      "[Epoch 17]  Loss: 1.02211\n",
      "[Epoch 18]  Loss: 1.01625\n"
     ]
    }
   ],
   "source": [
    "x_pred, reconstruct_data = iPerturb.model.RUN(datasets, iPerturb_model, svi, scheduler, epochs, hyper, raw, cuda, batch_size=100, if_likelihood=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "1. we visualize the UMAP plots before and after integration to demonstrate the effectiveness of iPerturb integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP before integration\n",
    "sc.pp.normalize_total(raw)\n",
    "sc.pp.log1p(raw)\n",
    "sc.pp.scale(raw)\n",
    "sc.tl.pca(raw)\n",
    "sc.pp.neighbors(raw, n_neighbors=30,n_pcs=50)\n",
    "sc.tl.umap(raw)\n",
    "sc.pl.umap(raw, color = 'groundtruth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP after integration\n",
    "sc.pp.neighbors(reconstruct_data, n_neighbors=30,n_pcs=50)\n",
    "sc.tl.umap(reconstruct_data)\n",
    "sc.pl.umap(reconstruct_data, color = 'groundtruth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP after integration by t_logits\n",
    "sc.pl.umap(reconstruct_data, color = 't_logits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. we calculate the EMD (Earth Mover's Distance) between the t_logits of the experimental and control groups, divided by cell type, to illustrate that the model's t_logits can explain the perturbation intensity at the cellular level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ot #### Python Optimal Transport: pip install POT\n",
    "from scipy.spatial.distance import cdist\n",
    "def calc_EMD(a,b):\n",
    "    distance_matrix = cdist(a, b)\n",
    "    a_weights = np.ones(a.shape[0]) / a.shape[0]\n",
    "    b_weights = np.ones(b.shape[0]) / b.shape[0]\n",
    "    # 计算EMD\n",
    "    emd_distance = ot.emd2(a_weights, b_weights, distance_matrix)\n",
    "\n",
    "    return emd_distance\n",
    "\n",
    "celltype = reconstruct_data.obs['celltype'].unique() \n",
    "condition = reconstruct_data.obs['condition'].unique() \n",
    "df_all = pd.DataFrame(columns=['celltype', 'emd'])  \n",
    "for key in celltype:\n",
    "        adata_subset = reconstruct_data[reconstruct_data.obs['celltype'] == key]\n",
    "        P = adata_subset[adata_subset.obs['batch'] == condition[0]].obsm['t_logits']\n",
    "        Q = adata_subset[adata_subset.obs['batch'] == condition[1]].obsm['t_logits']\n",
    "        emd = calc_EMD(P, Q)\n",
    "        df_all = df_all.append({'celltype': key, 'emd': emd}, ignore_index=True)  \n",
    "        print(f'{key} is done!')\n",
    "\n",
    "df_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MOBA_Billy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
